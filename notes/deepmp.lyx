#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass revtex4-1
\begin_preamble
\DeclareMathOperator\atanh{atanh}
\DeclareMathOperator\sign{sign}
\DeclareMathOperator\erfcx{erfcx}


\usepackage[T1]{fontenc}
\setcounter{secnumdepth}{3}
\usepackage{color}
\usepackage{babel}
\usepackage{mathrsfs}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
%\usepackage[unicode=true,pdfusetitle, bookmarks=true,bookmarksnumbered=false,bookmarksopen=false, breaklinks=false,pdfborder={0 0 0},pdfborderstyle={},backref=false,colorlinks=true]{hyperref}
\hypersetup{
 linkcolor=burntgreen,citecolor=red,urlcolor=burntorange}

\makeatletter

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.
\usepackage{color}
\definecolor{burntorange}{rgb}{0.8, 0.33, 0.0}
\definecolor{charcoal}{rgb}{0.21, 0.27, 0.31}
\definecolor{coolblack}{rgb}{0.0, 0.28, 0.49}
\definecolor{burntgreen}{rgb}{0.05, 0.45, 0.27}
\definecolor{burntblue}{rgb}{0.05, 0.27, 0.8}

\setcitestyle{authoryear,round}
\setlength{\bibsep}{2mm}

\makeatother

\bibliography{refs.bib}
\end_preamble
\use_default_options true
\begin_modules
initials
eqs-within-sections
figs-within-sections
tabs-within-sections
theorems-ams
\end_modules
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\float_placement H
\paperfontsize default
\spacing single
\use_hyperref true
\pdf_author "Fabrizio Pittorino"
\pdf_bookmarks true
\pdf_bookmarksnumbered true
\pdf_bookmarksopen false
\pdf_bookmarksopenlevel 1
\pdf_breaklinks false
\pdf_pdfborder true
\pdf_colorlinks true
\pdf_backref false
\pdf_pdfusetitle true
\pdf_quoted_options "pdfstartview=XYZ, plainpages=false, pdfpagelabels"
\papersize a4paper
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine natbib
\cite_engine_type authoryear
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\use_minted 0
\branch 1
\selected 0
\filename_suffix 0
\color #faf0e6
\end_branch
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 3cm
\topmargin 2.5cm
\rightmargin 3cm
\bottommargin 2.5cm
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Deep Message Passing
\end_layout

\begin_layout Abstract
Around the middle of the XVII century, a young man called Baruch Spinoza
 was dealing with a problem tougher than yours: finding the essence of God
 Himself.
\end_layout

\begin_layout Abstract
\begin_inset CommandInset toc
LatexCommand tableofcontents

\end_inset


\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Section
Learning through message passing
\end_layout

\begin_layout Standard
TODO ENTIRE SECTION.
\end_layout

\begin_layout Standard
In this deep inference problem, we assume that a signal with prior 
\begin_inset Formula $P^{\text{in}}$
\end_inset

 is fed to a deep feedforward networks with 
\begin_inset Formula $L+1$
\end_inset

 layers of weights 
\begin_inset Formula $\boldsymbol{W}^{\ell}\in\mathbb{R}^{N_{\ell+1}\times N_{\ell}},\ell=0,\dots,L$
\end_inset

 and biases 
\begin_inset Formula $\boldsymbol{b}^{\ell}\in\text{\ensuremath{\mathbb{R}^{N_{\ell+1}}}}$
\end_inset

.
 The signal is propagated through stochastic neuron layers described by
 probability distributions 
\begin_inset Formula $P^{\ell}$
\end_inset

 conditioned on the preactivations, therefore we have the following Markov
 chain:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align}
\boldsymbol{x}^{0} & \sim P^{\text{in}}\\
\boldsymbol{x}^{\ell+1} & \sim P^{\ell+1}\left(\bullet\,|\,\boldsymbol{W}^{\ell}\boldsymbol{x}^{\ell}+\boldsymbol{b}^{\ell}\right)\qquad\ell=0,\dots,L
\end{align}

\end_inset


\end_layout

\begin_layout Standard
Only 
\begin_inset Formula $\boldsymbol{y}=\boldsymbol{x}^{L+1}$
\end_inset

 is observed, and the task is to reconstruct the original signal 
\begin_inset Formula $\boldsymbol{x}^{0}$
\end_inset

.
 The posterior distribution 
\begin_inset Formula $p(\boldsymbol{x}^{0:L})=P(\boldsymbol{x}^{0:L}\,|\,\boldsymbol{x}^{L+1}=\boldsymbol{y})$
\end_inset

 reads
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align}
p(\boldsymbol{x}^{0:L}) & \propto\prod_{\ell=0}^{L}\prod_{k=1}^{N_{\ell+1}}\,P_{k}^{\ell+1}\left(x_{k}^{\ell+1}\ \bigg|\ \sum_{i=1}^{N_{\ell}}W_{ki}^{\ell}x_{i}^{\ell}\right)\ \prod_{k=1}^{N_{0}}P_{k}^{\text{in}}(x_{k}^{0}),
\end{align}

\end_inset


\end_layout

\begin_layout Standard
Typical channels are given by deterministic elementwise activation function
 
\begin_inset Formula $f_{\ell}(z)$
\end_inset

 (e.g.
 
\begin_inset Formula $f_{\ell}(z)=\sign(x)$
\end_inset

 or 
\begin_inset Formula $f_{\ell}(z)=\text{relu}(z)=\max(0,z)$
\end_inset

), combined with Gaussian additive pre-activation noise with variance 
\begin_inset Formula $\sigma^{2}$
\end_inset

.
 In such cases we have
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
P_{k}^{\ell}(x\,|\,z)=\int D\xi\,\delta\left(x-f_{\ell}(z+\sigma_{k}^{\ell}\,\xi)\right)
\]

\end_inset


\end_layout

\begin_layout Standard
We also call 
\begin_inset Formula $\alpha_{\ell}=N_{\ell+1}/N_{\ell}$
\end_inset

 the layer 
\emph on
expansion ratio.
\end_layout

\begin_layout Section
Belief Propagation
\end_layout

\begin_layout Subsection
BP updates
\end_layout

\begin_layout Standard
The AMP equations have been derived from the BP equation in the Appendix.
 First we introduce the neuron scalar entropy functions:
\begin_inset Formula 
\begin{align}
\varphi_{k}^{0}(B,A) & =\log\int\mathrm{d}x\ e^{-\frac{1}{2}A^{2}x^{2}+Bx}\,P_{k}^{\text{in}}(x)\\
\varphi_{k}^{\ell}(B,A,\omega,V) & =\log\int\mathrm{d}x\,\mathrm{d}z\ e^{-\frac{1}{2}A^{2}x^{2}+Bx}\,P_{k}^{\ell}\left(x|z\right)e^{-\frac{(\omega-z)^{2}}{2V}}\qquad\ell=1,\dots,L\\
\varphi_{k}^{L+1}(\omega,V,y) & =\log\int\mathrm{d}z\ P_{k}^{L+1}\left(y|z\right)e^{-\frac{(\omega-z)^{2}}{2V}}\\
\psi_{ki}^{\ell}(H,G) & =\log\int\mathrm{d}w\ e^{-\frac{1}{2}G^{2}w^{2}+Hw}\,P_{ki}^{\text{\ensuremath{\ell}}}(w)
\end{align}

\end_inset


\end_layout

\begin_layout Standard
For convenience define 
\begin_inset Formula $\varphi_{i}^{0,t}=\varphi_{i}^{0}\left(B_{i}^{0,t},A_{i}^{0,t}\right)$
\end_inset

 and 
\begin_inset Formula $\varphi_{i}^{\ell,t}=\varphi_{i}^{\ell}\left(B_{i}^{\ell,t},A_{i}^{\ell,t},\omega_{i}^{\ell-1,t},V_{i}^{\ell-1,t}\right)$
\end_inset

 and 
\begin_inset Formula $\varphi_{i}^{L+1,t}=\varphi_{i}^{L+1}\left(\omega_{i}^{L,t},V_{i}^{L,t},y_{i}\right)$
\end_inset

.
 
\end_layout

\begin_layout Standard
Then, we can decompose the BP update rules in a forward and a backward step.
\end_layout

\begin_layout Paragraph
Forward pass.
\end_layout

\begin_layout Standard
As the initial condition for the iterations, we set to zero the following
 quantities: 
\begin_inset Formula $B_{i}^{\ell,t=0}=0,A_{i}^{\ell,t=0}=0$
\end_inset

 and 
\begin_inset Formula $g_{k}^{\ell,t=0}=0$
\end_inset

.
 The following iterations hold at time 
\begin_inset Formula $t\geq1$
\end_inset

.
 In the FORWARD pass, starting from 
\begin_inset Formula $\ell=0$
\end_inset

 and up to 
\begin_inset Formula $\ell=L$
\end_inset

, we have
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align}
\hat{x}_{ia\to k}^{\ell,t} & =\partial_{B}\varphi_{ia\to k}^{\ell}\left(B_{ia\to k}^{\ell,t-1},A_{ia}^{\ell,t-1},\omega_{ia}^{\ell-1,t},V_{ia}^{\ell-1,t}\right)\\
\Delta_{ia\to k}^{\ell+1,t} & =\partial_{B}^{2}\varphi_{ia\to k}^{\ell+1,t}\\
m_{ki\to a}^{\ell,t} & =\partial_{H}\psi_{ki}^{\ell}(H_{ki\to a}^{t-1},G_{ki}^{t-1})\\
\sigma_{ki\to a}^{\ell,t} & =\partial_{H}^{2}\psi_{ki}^{\ell}(H_{ki\to a}^{t-1},G_{ki}^{t-1})\\
V_{ka}^{\ell,t} & =\sum_{i}\left(\left(m_{ki\to a}^{\ell,t}\right)^{2}\Delta_{ia\to k}^{\ell,t}+\Sigma_{ki\to a}^{\ell,t}(\hat{x}_{ia\to k}^{\ell,t})^{2}+\sigma_{ki\to a}^{\ell,t}\Delta_{ia\to k}^{\ell,t}\right)\label{eq:upd-V-2}\\
\omega_{ka\to i}^{\ell,t} & =\sum_{i'\neq i}m_{ki\to a}^{\ell,t}\hat{x}_{ia\to k}^{\ell,t}\label{eq:upd-w-2}
\end{align}

\end_inset


\end_layout

\begin_layout Standard
Here 
\begin_inset Formula $V^{\ell}$
\end_inset

 and 
\begin_inset Formula $\omega^{\ell}$
\end_inset

 are computed as a function of the previous layer values 
\begin_inset Formula $V^{\ell-1}$
\end_inset

 and 
\begin_inset Formula $\omega^{\ell-1}$
\end_inset

.
 
\end_layout

\begin_layout Paragraph
Backward pass.
\end_layout

\begin_layout Standard
In the BACKWARD sweep, starting from 
\begin_inset Formula $\ell=L$
\end_inset

 and down to 
\begin_inset Formula $\ell=0$
\end_inset

, we have 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align}
g_{ka\to i}^{\ell,t} & =\partial_{\omega}\varphi_{ka\to i}^{\ell+1}\left(B_{ka}^{\ell+1,t},A_{ka}^{\ell+1,t},\omega_{ka\to i}^{\ell,t},V_{ka}^{\ell,t}\right)\label{eq:upd-g-2}\\
\Gamma_{ka\to i}^{\ell,t} & =-\partial_{\omega}^{2}\varphi_{ka\to i}^{\ell+1,t}\label{eq:upd-dwg-2}\\
A_{ia}^{\ell,t} & =\sum_{k}\left((m_{ki\to a}^{\ell,t})^{2}+\sigma_{ki\to a}^{\ell,t}\right)\Gamma_{ka\to i}^{\ell,t}-\sigma_{ki\to a}^{\ell,t}\left(g_{ka\to i}^{\ell,t}\right)^{2}\label{eq:upd-A-2}\\
B_{ia\to k}^{\ell,t} & =\sum_{k'\neq k}m_{k'i\to a}^{\ell}g_{k'a\to i}^{\ell,t}\label{eq:upd-B-2}\\
G_{ki}^{\ell,t} & =\sum_{a}\left((\hat{x}_{ia\to k}^{\ell,t})^{2}+\Delta_{ia\to k}\right)\Gamma_{ka\to i}^{\ell,t}-\Delta_{ia\to k}\left(g_{ka\to i}^{\ell,t}\right)^{2}\\
H_{ki\to a} & =\sum_{a'\neq a}\hat{x}_{ia'\to k}^{\ell,t}g_{ka'\to i}^{\ell,t}
\end{align}

\end_inset


\end_layout

\begin_layout Standard
Notice that 
\begin_inset Formula $A^{\ell}$
\end_inset

 and 
\begin_inset Formula $B^{\ell}$
\end_inset

 are computed as as a function of the 
\begin_inset Formula $A^{\ell+1},B^{\ell+1}$
\end_inset

 of the layer above, with the initial condition given by the output 
\begin_inset Formula $\boldsymbol{x}^{L+1}=\boldsymbol{y}$
\end_inset

 on the top layer.
\end_layout

\begin_layout Subsection
Approximate Message Passing
\end_layout

\begin_layout Paragraph
Forward pass.
\end_layout

\begin_layout Standard
As the initial condition for the iterations, we set to zero the following
 quantities: 
\begin_inset Formula $B_{i}^{\ell,t=0}=0,A_{i}^{\ell,t=0}=0$
\end_inset

 and 
\begin_inset Formula $g_{k}^{\ell,t=0}=0$
\end_inset

.
 The following iterations hold at time 
\begin_inset Formula $t\geq1$
\end_inset

.
 In the FORWARD pass, starting from 
\begin_inset Formula $\ell=0$
\end_inset

 and up to 
\begin_inset Formula $\ell=L$
\end_inset

, we have
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align}
\hat{x}_{ia}^{\ell,t} & =\partial_{B}\varphi_{ia}^{\ell,t^{-}}\label{eq:upd-h-2-1}\\
\Delta_{ia}^{\ell,t} & =\partial_{B}^{2}\varphi_{ia}^{\ell,t^{-}}\label{eq:upd-sigma-2-1}\\
m_{ki}^{\ell,t} & =\partial_{H}\psi_{ki}^{\ell,t^{-}}\\
\sigma_{ki}^{\ell,t} & =\partial_{H}^{2}\psi_{ki}^{\ell,t^{-}}\\
V_{ka}^{\ell,t} & =\sum_{i}\left(\left(m_{ki}^{\ell,t}\right)^{2}\Delta_{ia}^{\ell,t}+\sigma_{ki}^{\ell,t}(\hat{x}_{ia}^{\ell,t})^{2}+\sigma_{ki}^{\ell,t}\Delta_{ia}^{\ell,t}\right)\label{eq:upd-V-2-1}\\
\omega_{ka}^{\ell,t} & =\sum_{i}m_{ki}^{\ell,t}\hat{x}_{ia}^{\ell,t}+TODO:onsagsize(x)er\label{eq:upd-w-2-1}
\end{align}

\end_inset


\end_layout

\begin_layout Standard
Here 
\begin_inset Formula $V^{\ell}$
\end_inset

 and 
\begin_inset Formula $\omega^{\ell}$
\end_inset

 are computed as a function of the previous layer values 
\begin_inset Formula $V^{\ell-1}$
\end_inset

 and 
\begin_inset Formula $\omega^{\ell-1}$
\end_inset

.
 
\end_layout

\begin_layout Paragraph
Backward pass.
\end_layout

\begin_layout Standard
In the BACKWARD sweep, starting from 
\begin_inset Formula $\ell=L$
\end_inset

 and up to 
\begin_inset Formula $\ell=0$
\end_inset

, we have 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align}
g_{ka}^{\ell,t} & =\partial_{\omega}\varphi_{ka}^{\ell+1,t}\label{eq:upd-g-2-1}\\
\Gamma_{ka}^{\ell,t} & =-\partial_{\omega}^{2}\varphi_{ka}^{\ell+1,t}\label{eq:upd-dwg-2-1}\\
A_{ia}^{\ell,t} & =\sum_{k}\left((m_{ki}^{\ell,t})^{2}+\sigma_{ki}^{\ell,t}\right)\Gamma_{ka}^{\ell,t}-\sigma_{ki}^{\ell,t}\left(g_{ka}^{\ell,t}\right)^{2}\label{eq:upd-A-2-1}\\
B_{ia}^{\ell,t} & =\sum_{k}m_{ki}^{\ell}g_{ka}^{\ell,t}+TODO:onsager\label{eq:upd-B-2-1}\\
G_{ki}^{\ell,t} & =\sum_{a}\left((\hat{x}_{ia}^{\ell,t})^{2}+\Delta_{ia}\right)\Gamma_{ka}^{\ell,t}-\Delta_{ia}\left(g_{ka}^{\ell,t}\right)^{2}\\
H_{ki} & =\sum_{a}\hat{x}_{ia}^{\ell,t}g_{ka}^{\ell,t}+TODO:onsager
\end{align}

\end_inset


\end_layout

\begin_layout Standard
Notice that 
\begin_inset Formula $A^{\ell}$
\end_inset

 and 
\begin_inset Formula $B^{\ell}$
\end_inset

 are computed as as a function of the 
\begin_inset Formula $A^{\ell+1},B^{\ell+1}$
\end_inset

 of the layer above, with the initial condition given by the output 
\begin_inset Formula $\boldsymbol{x}^{L+1}=\boldsymbol{y}$
\end_inset

 on the top layer.
\end_layout

\begin_layout Section
ArgMax layer
\end_layout

\begin_layout Standard
In order to perform multi-class classification, we have perform an argmax
 operation.
 Call 
\begin_inset Formula $z_{k}$
\end_inset

, for 
\begin_inset Formula $k=1,\dots,K$
\end_inset

, the Gaussian random variables output of the last layer of the network
 in correspondence of some input 
\begin_inset Formula $\boldsymbol{x}$
\end_inset

.
 Assuming the correct label is class 
\begin_inset Formula $k$
\end_inset

, the effective partition function corresponding to the output constraint
 reads
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align}
Z_{k^{*}} & =\int\prod_{k}dz_{k}\,\mathcal{N}(z_{k};\omega_{k},V_{k})\ \prod_{k\neq k^{*}}\theta(z_{k^{*}}-z_{k})\\
 & =\int dz_{k^{*}}\,\mathcal{N}(z_{k^{*}};\omega_{k^{*}},V_{k^{*}})\ \prod_{k\neq k^{*}}H\left(-\frac{z_{k^{*}}-\omega_{k}}{\sqrt{V_{k}}}\right)
\end{align}

\end_inset


\end_layout

\begin_layout Standard
This last integral is intractable, therefore we have to resort to approximations.
\end_layout

\begin_layout Subsection
Approach 1: Jensen Inequality
\end_layout

\begin_layout Standard
Using the Jensen inequality we obtain
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align}
\phi_{k^{*}}= & \log Z_{k^{*}}=\log\mathbb{E}_{z\sim\mathcal{N}(\omega_{k^{*}},V_{k^{*}})}\prod_{k\neq k^{*}}H\left(-\frac{z-\omega_{k}}{\sqrt{V_{k}}}\right)\\
 & \geq\sum_{k\neq k^{*}}\mathbb{E}_{z\sim\mathcal{N}(\omega_{k^{*}},V_{k^{*}})}\log H\left(-\frac{z-\omega_{k}}{\sqrt{V_{k}}}\right)
\end{align}

\end_inset


\end_layout

\begin_layout Standard
Reparameterizing the expections we have 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\tilde{\phi}_{k^{*}}=\sum_{k\neq k^{*}}\mathbb{E}_{\epsilon\sim\mathcal{N}(0,1)}\log H\left(-\frac{\omega_{k^{*}}+\epsilon\sqrt{V_{k^{*}}}-\omega_{k}}{\sqrt{V_{k}}}\right)\label{eq:argmax1}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
The derivative 
\begin_inset Formula $\partial_{\omega_{k}}\tilde{\phi}_{k^{*}}$
\end_inset

 and 
\begin_inset Formula $\partial_{\omega_{k}}^{2}\tilde{\phi}_{k^{*}}$
\end_inset

 that we need can then be estimated by sampling (once?) 
\begin_inset Formula $\epsilon$
\end_inset

.
\end_layout

\begin_layout Subsection
Approach 2: Jensen again
\end_layout

\begin_layout Standard
A further simplification is obtained by applying Jensen inequality again
 to 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:argmax1"
plural "false"
caps "false"
noprefix "false"

\end_inset

 but in the opposite direction.
 We have the new effective free energy
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align}
\tilde{\phi}_{k^{*}} & =\sum_{k\neq k^{*}}\log\mathbb{E}_{\epsilon\sim\mathcal{N}(0,1)}H\left(-\frac{\omega_{k^{*}}+\epsilon\sqrt{V_{k^{*}}}-\omega_{k}}{\sqrt{V_{k}}}\right)\\
 & =\sum_{k\neq k^{*}}\log H\left(-\frac{\omega_{k^{*}}-\omega_{k}}{\sqrt{V_{k}+V_{k^{*}}}}\right)
\end{align}

\end_inset


\end_layout

\begin_layout Standard
This gives, for 
\begin_inset Formula $k\neq k^{*}$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\partial_{\omega_{k}}\tilde{\phi}_{k^{*}}=\begin{cases}
-\frac{1}{\sqrt{V_{k}+V_{k^{*}}}}\,GH\left(-\frac{\omega_{k^{*}}-\omega_{k}}{\sqrt{V_{k}+V_{k^{*}}}}\right) & k\neq k^{*}\\
\sum_{k'\neq k^{*}}\frac{1}{\sqrt{V_{k'}+V_{k^{*}}}}\,GH\left(-\frac{\omega_{k^{*}}-\omega_{k'}}{\sqrt{V_{k'}+V_{k^{*}}}}\right) & k=k^{*}
\end{cases}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
Notice that 
\begin_inset Formula $\partial_{\omega_{k^{*}}}\tilde{\phi}_{k^{*}}=-\sum_{k\neq k^{*}}\partial_{\omega_{k}}\tilde{\phi}_{k^{*}}$
\end_inset

.
 In last formulas we used the definition
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
GH(x)=\frac{G(x)}{H(x)}=\frac{\sqrt{2/\pi}}{\erfcx(x/2)}
\end{equation}

\end_inset


\end_layout

\begin_layout Section
Numerical results
\end_layout

\begin_layout Standard
In this section we study multi-layer perceptrons (MLPs) on a binary classificati
on task on the Fashion-MNIST dataset (divided arbitrarily in two classes:
 the first/last 5 classes in the original dataset respectively represent
 the first/second class in the binary classification task).
 We perform experiments on the whole dataset and with a MLP with two hidden
 layers of size 
\begin_inset Formula $101$
\end_inset

 (apart from the sections in which we modify the architecture or the dataset
 size, or otherwise stated).
\end_layout

\begin_layout Standard
NB: in all figures the title reports 
\begin_inset Quotes eld
\end_inset

MNIST
\begin_inset Quotes erd
\end_inset

 while it should be 
\begin_inset Quotes eld
\end_inset

Fashion-MNIST
\begin_inset Quotes erd
\end_inset

.
 
\end_layout

\begin_layout Standard
\begin_inset Formula $M$
\end_inset

 and 
\begin_inset Formula $P$
\end_inset

 are the same parameter (the dataset size).
\end_layout

\begin_layout Subsection
Experiments on Fashion-MNIST
\end_layout

\begin_layout Standard
We compare the BP family with binary-SGD without biases and without additional
 parameters for batch normalization.
 In order to keep the pre-activations of each hidden layer normalized we
 rescale them by 
\begin_inset Formula $\frac{1}{\sqrt{N_{\text{in}}}}$
\end_inset

 where 
\begin_inset Formula $N_{\text{in}}$
\end_inset

 is the size of the previous hidden layer (or the input size in the case
 of the preactivations afferent to the first hidden layer).
\end_layout

\begin_layout Subsubsection
Good parameters: batch-size 1 and batch-size 128
\end_layout

\begin_layout Standard
We report that it is possible to find values of the hyper-parameters such
 that the BP family has final train/test error comparable with SGD.
 In our experiments this holds for generic dataset sizes and batch-sizes,
 and for generic MLPs (generic depth / hidden layer size).
\end_layout

\begin_layout Standard
In summary, up to now the best hyper-parameters (apart from 
\begin_inset Formula $P$
\end_inset

, batch-size and 
\begin_inset Formula $\rho$
\end_inset

 that has to be tuned similarly to the learning rate for SGD) are:
\end_layout

\begin_layout Itemize
\begin_inset Formula $\psi=0.8$
\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $\epsilon init=1$
\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $maxiters=1$
\end_inset

 (
\begin_inset Formula $r=0$
\end_inset

)
\end_layout

\begin_layout Standard
\align center
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename figures/deepMP_bs1_K[784, 501, 501, 501, 1]_rho1.0e-6_ψ_0.8_P60000.0_maxiters_1_r0.0_ϵinit_1.0_.png
	scale 70

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename figures/pasted/pasted25.png
	scale 25

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Left panel: MLP with 3 hidden layers with 501 hidden units each, batch-size=1
 on the Fashion-MNIST dataset.
 Right panel: MLP with 2 hidden layers with 101 hidden units each, batch-size=12
8 on the Fashion-MNIST dataset.
 Here we have selected some 
\begin_inset Quotes eld
\end_inset

good
\begin_inset Quotes erd
\end_inset

 values of the parameters.
 
\series bold
NB: in all figures the upper inset is the self-overlap of the first layer,
 while the lower inset is the mean overlap of the couples of sub-perceptrons
 in the first layer.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Varying batch-size: computational performance
\end_layout

\begin_layout Standard
Here we vary only the batchsize (per completezza farlo per un buon set degli
 altri iperparametri), in order to compare the performance (time) of BP
 with binary-SGD (both on GPUs).
\end_layout

\begin_layout Standard
The command to reproduce the experiments in this section is:
\end_layout

\begin_layout Standard

\color blue
run_experiment(9; M=Int(6e4), batchsize=batchsize, usecuda=true, gpu_id=0,
 ρ=1+1e-5, ψ=0.5, lay=lay, epochs=100)
\end_layout

\begin_layout Standard
with batchsize={1,16,128,1024} and lay={:bp, :bpi, :tap}.
\end_layout

\begin_layout Standard
\align center
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename figures/deepMP_bs1_K[784, 101, 101, 1]_comparison.png
	scale 50

\end_inset


\begin_inset Graphics
	filename figures/deepMP_bs16_K[784, 101, 101, 1]_comparison.png
	scale 50

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename figures/deepMP_bs128_K[784, 101, 101, 1]_comparison.png
	scale 50

\end_inset


\begin_inset Graphics
	filename figures/deepMP_bs1024_K[784, 101, 101, 1]_comparison.png
	scale 50

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Comparison of BP, TAP, BPI, SGD varying the batchsize (upper left: bs=1;
 upper right: bs=16, lower left: bs=128; lower right=1024).
 The parameter 
\begin_inset Formula $\rho-1$
\end_inset

 is fixed in all experiments to 
\begin_inset Formula $10^{-5}$
\end_inset

.
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\align center
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename figures/deepMP_times_K[784, 101, 101, 1].png
	scale 75

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Algorithms time scaling with the batchsize.
 The reported time refers to one epoch for each algorithm.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Experiments on Fashion-MNIST: parameters
\end_layout

\begin_layout Standard
Here we stick to batchsize 128 and we vary the other hyperparameters.
 However, we expect that some of the parameters scale with the batchsize
 (in particular we expect 
\begin_inset Formula $\rho-1\propto\frac{\text{batchsize}}{\text{data set size}}$
\end_inset

).
\end_layout

\begin_layout Standard
The command to reproduce the experiments is along the lines of:
\end_layout

\begin_layout Standard
run_experiment(9; usecuda=true, gpu_id=0, epochs=100, lay={:bp, :bpi, :tap},
 batchsize=128, ρ=p[2], ψ=0.5, M=Int(6e4), maxiters=1, r=0., K=[28*28,101,101,1])
\end_layout

\begin_layout Subsubsection
Varying 
\begin_inset Formula $\rho$
\end_inset


\end_layout

\begin_layout Standard
ρs = [-1e-1, -1e-5, 0., 1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1] .+ 1.
 
\end_layout

\begin_layout Standard
For 
\begin_inset Formula $bs=128$
\end_inset

 we choose 
\begin_inset Formula $\rho-1=10^{-4}$
\end_inset

.
 The command is the same as the one in the first paragraph of section B
 except for 
\begin_inset Formula $\rho$
\end_inset

.
\end_layout

\begin_layout Standard
\align center
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename figures/deepMP_bs128_K[784, 101, 101, 1]_rho-0.1_ψ_0.5_P60000.0_maxiters_1_r0.0_ϵinit_1.0_.png
	scale 50

\end_inset


\begin_inset Graphics
	filename figures/deepMP_bs128_K[784, 101, 101, 1]_rho-1.0e-5_ψ_0.5_P60000.0_maxiters_1_r0.0_ϵinit_1.0_.png
	scale 50

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename figures/pasted/pasted1.png
	scale 20

\end_inset


\begin_inset Graphics
	filename figures/pasted/pasted2.png
	scale 20

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename figures/pasted/pasted3.png
	scale 20

\end_inset


\begin_inset Graphics
	filename figures/pasted/pasted4.png
	scale 20

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
MLP with 2 hidden layers with 101 hidden units each, batch-size=128 on the
 Fashion-MNIST dataset.
 We vary the parameter 
\begin_inset Formula $\rho$
\end_inset

.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\align center
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset Graphics
	filename figures/pasted/pasted5.png
	scale 20

\end_inset


\begin_inset Graphics
	filename figures/pasted/pasted6.png
	scale 20

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename figures/pasted/pasted7.png
	scale 20

\end_inset


\begin_inset Graphics
	filename figures/pasted/pasted8.png
	scale 20

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
MLP with 2 hidden layers with 101 hidden units each, batch-size=128 on the
 Fashion-MNIST dataset.
 We vary the parameter 
\begin_inset Formula $\rho$
\end_inset

.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Varying damping 
\begin_inset Formula $\psi$
\end_inset


\end_layout

\begin_layout Standard
ψs = [0:0.2:1;] 
\begin_inset Formula $\cup$
\end_inset

 [0.9, 0.99, 0.999, 0.9999]
\end_layout

\begin_layout Standard
For 
\begin_inset Formula $bs=128$
\end_inset

 and 
\begin_inset Formula $\rho-1=10^{-4}$
\end_inset

 we choose 
\begin_inset Formula $\psi=0.8$
\end_inset

.
 The command is the same as the one in the first paragraph of section B
 except for 
\begin_inset Formula $\rho$
\end_inset

 and 
\begin_inset Formula $\psi$
\end_inset

.
\end_layout

\begin_layout Standard
\align center
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset Graphics
	filename figures/pasted/pasted9.png
	scale 20

\end_inset


\begin_inset Graphics
	filename figures/pasted/pasted10.png
	scale 20

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename figures/pasted/pasted11.png
	scale 20

\end_inset


\begin_inset Graphics
	filename figures/pasted/pasted12.png
	scale 20

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename figures/pasted/pasted13.png
	scale 20

\end_inset


\begin_inset Graphics
	filename figures/pasted/pasted14.png
	scale 20

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
MLP with 2 hidden layers with 101 hidden units each, batch-size=128 on the
 Fashion-MNIST dataset.
 We vary the parameter 
\begin_inset Formula $\psi$
\end_inset

.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\align center
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset Graphics
	filename figures/pasted/pasted15.png
	scale 20

\end_inset


\begin_inset Graphics
	filename figures/pasted/pasted16.png
	scale 20

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename figures/pasted/pasted17.png
	scale 20

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
MLP with 2 hidden layers with 101 hidden units each, batch-size=128 on the
 Fashion-MNIST dataset.
 We vary the parameter 
\begin_inset Formula $\psi$
\end_inset

.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Varying initial weights
\end_layout

\begin_layout Standard
ϵinits = [0., 1e-3, 1e-2, 1e-1, 5e-1, 1e0]
\end_layout

\begin_layout Standard
\align center
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset Graphics
	filename figures/pasted/pasted19.png
	scale 20

\end_inset


\begin_inset Graphics
	filename figures/pasted/pasted24.png
	scale 20

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename figures/pasted/pasted20.png
	scale 20

\end_inset


\begin_inset Graphics
	filename figures/pasted/pasted21.png
	scale 20

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename figures/pasted/pasted22.png
	scale 20

\end_inset


\begin_inset Graphics
	filename figures/pasted/pasted23.png
	scale 20

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
MLP with 2 hidden layers with 101 hidden units each, batch-size=128 on the
 Fashion-MNIST dataset.
 We vary the parameter 
\begin_inset Formula $\epsilon$
\end_inset


\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none
init
\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\xout default
\uuline default
\uwave default
\noun default
\color inherit
.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Varying dataset size
\end_layout

\begin_layout Standard
Ms = [Int(1e2), Int(1e3), Int(1e4), Int(6e4)] 
\end_layout

\begin_layout Standard
bs = [Int(1e0), Int(1e1), Int(1e2), Int(6e2)]
\end_layout

\begin_layout Standard
We fix the ratio 
\begin_inset Formula $\frac{\text{dataset size}}{\text{batch size}}=\frac{M}{b}=10^{2}$
\end_inset

.
\end_layout

\begin_layout Standard
\align center
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename figures/pasted/pasted51.png
	scale 20

\end_inset


\begin_inset Graphics
	filename figures/pasted/pasted52.png
	scale 20

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename figures/pasted/pasted49.png
	scale 20

\end_inset


\begin_inset Graphics
	filename figures/pasted/pasted50.png
	scale 20

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
MLP with 2 hidden layers with 101 hidden units each, batch-size=128 on the
 Fashion-MNIST dataset.
 We vary the parameter 
\begin_inset Formula $M$
\end_inset

 (with 
\begin_inset Formula $\frac{M}{b}=10^{2}$
\end_inset

).
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Varying 
\begin_inset Formula $maxiters$
\end_inset


\end_layout

\begin_layout Standard
maxiterss = [1, 10, 50, 100].
 Here also time is interesting.
 It is not very favorable in terms of time compared to SGD to choose 
\begin_inset Formula $maxiters>1$
\end_inset

, however it is interesting how many iterations are necessary for convergence.
\end_layout

\begin_layout Standard
\align center
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset Graphics
	filename figures/pasted/pasted39.png
	scale 20

\end_inset


\begin_inset Graphics
	filename figures/pasted/pasted40.png
	scale 20

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename figures/pasted/pasted41.png
	scale 20

\end_inset


\begin_inset Graphics
	filename figures/pasted/pasted42.png
	scale 20

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
MLP with 2 hidden layers with 101 hidden units each, batch-size=128 on the
 Fashion-MNIST dataset.
 We vary the parameter 
\begin_inset Formula $maxiters$
\end_inset

.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Varying 
\begin_inset Formula $r$
\end_inset


\end_layout

\begin_layout Standard
rs = [0:0.2:1.2;] (for maxiters=10).
 Da rifare.
\end_layout

\begin_layout Standard
\align center
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename figures/pasted/pasted40.png
	scale 20

\end_inset


\begin_inset Graphics
	filename figures/pasted/pasted43.png
	scale 20

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
MLP with 2 hidden layers with 101 hidden units each, batch-size=128 on the
 Fashion-MNIST dataset.
 We fix 
\begin_inset Formula $maxiters=10$
\end_inset

 (just to be faster) and we vary the parameter 
\begin_inset Formula $r$
\end_inset

.
 I vari valori di 
\begin_inset Formula $r\ne0$
\end_inset

 non hanno proprio funzionato (sembra siano tutti NaN) - gli altri parametri
 vanno cambiati.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Varying the number of layers and the size of the hidden layer
\end_layout

\begin_layout Standard
We fix some reasonable values of the hyperparameters (see previous experiments,
 in particular: 
\begin_inset Formula $P=6e4$
\end_inset

, 
\begin_inset Formula $bs=128$
\end_inset

, 
\begin_inset Formula $\psi=0.8$
\end_inset

, 
\begin_inset Formula $\epsilon init=1$
\end_inset

, 
\begin_inset Formula $maxiters=1$
\end_inset

, 
\begin_inset Formula $r=0$
\end_inset

) and want to check if the algorithm converges (and todo the time scaling).
\end_layout

\begin_layout Standard
\align center
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset Graphics
	filename figures/pasted/pasted26.png
	scale 20

\end_inset


\begin_inset Graphics
	filename figures/pasted/pasted27.png
	scale 20

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename figures/pasted/pasted28.png
	scale 20

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
MLP with 1 hidden layer with 101/501/1001 hidden units each, batch-size=128
 on the Fashion-MNIST dataset.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\align center
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset Graphics
	filename figures/pasted/pasted29.png
	scale 20

\end_inset


\begin_inset Graphics
	filename figures/pasted/pasted30.png
	scale 20

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename figures/pasted/pasted31.png
	scale 20

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
MLP with 2 hidden layers with 101/501/1001 hidden units each, batch-size=128
 on the Fashion-MNIST dataset.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\align center
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset Graphics
	filename figures/pasted/pasted32.png
	scale 20

\end_inset


\begin_inset Graphics
	filename figures/pasted/pasted33.png
	scale 20

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename figures/pasted/pasted34.png
	scale 20

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
MLP with 3 hidden layers with 101/501/1001 hidden units each, batch-size=128
 on the Fashion-MNIST dataset.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Experiments on RFM
\end_layout

\begin_layout Standard
Here we present results with the Random Features Model, concerning in particular
 the permutation symmetry.
 In order to investigate the role of the permutation symmetry we present
 results on the fully connected committee machine (1 hidden layer network
 learning only the first weight layer, with the weights of the second layer
 fixed to all ones).
\end_layout

\begin_layout Standard
The teacher in the latent space is a perceptron (check).
\end_layout

\begin_layout Standard
\align center
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename figures/rfm_bp.png
	scale 25

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Overlap varying 
\begin_inset Formula $N$
\end_inset

 in the RFM, fully connected committee machine with various algorithms.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\end_body
\end_document
