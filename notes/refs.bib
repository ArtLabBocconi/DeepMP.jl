% rigorous works
@article{Talagrand2006,
	title={The Parisi formula},
	author={Talagrand, Michel},
	journal={Annals of mathematics},
	pages={221--263},
	doi={10.4007/annals.2006.163.221},
	year={2006},
	publisher={JSTOR}
}

@Article{GuerraToninelli2002,
	author="Guerra, Francesco
	and Toninelli, Fabio Lucio",
	title="The Thermodynamic Limit in Mean Field Spin Glass Models",
	journal="Communications in Mathematical Physics",
	year="2002",
	month="Sep",
	day="01",
	volume="230",
	number="1",
	pages="71--79",
	publisher={Springer},
	abstract="{\enspace}We present a simple strategy in order to show the existence and uniqueness of the infinite volume limit of thermodynamic quantities, for a large class of mean field disordered models, as for example the Sherrington-Kirkpatrick model, and the Derrida p-spin model. The main argument is based on a smooth interpolation between a large system, made of N spin sites, and two similar but independent subsystems, made of N                                          1                 and N                                          2                 sites, respectively, with N                                          1                                        +N                                          2                                        =N. The quenched average of the free energy turns out to be subadditive with respect to the size of the system. This gives immediately convergence of the free energy per site, in the infinite volume limit. Moreover, a simple argument, based on concentration of measure, gives the almost sure convergence, with respect to the external noise. Similar results hold also for the ground state energy per site.",
	issn="1432-0916",
	doi="10.1007/s00220-002-0699-y",
	url="https://doi.org/10.1007/s00220-002-0699-y"
}


@Article{Guerra2003,
	author="Guerra, Francesco",
	title="Broken Replica Symmetry Bounds in the Mean Field Spin Glass Model",
	journal="Communications in Mathematical Physics",
	year="2003",
	month="Feb",
	day="01",
	volume="233",
	number="1",
	pages="1--12",
	abstract="{\enspace}By using a simple interpolation argument, in previous work we have proven the existence of the thermodynamic limit, for mean field disordered models, including the Sherrington-Kirkpatrick model, and the Derrida p-spin model. Here we extend this argument in order to compare the limiting free energy with the expression given by the Parisi Ansatz, and including full spontaneous replica symmetry breaking. Our main result is that the quenched average of the free energy is bounded from below by the value given in the Parisi Ansatz, uniformly in the size of the system. Moreover, the difference between the two expressions is given in the form of a sum rule, extending our previous work on the comparison between the true free energy and its replica symmetric Sherrington-Kirkpatrick approximation. We give also a variational bound for the infinite volume limit of the ground state energy per site.",
	issn="1432-0916",
	doi="10.1007/s00220-002-0773-5",
	url="https://doi.org/10.1007/s00220-002-0773-5"
}


@inproceedings{dia2016mutual,
  title={Mutual information for symmetric rank-one matrix estimation: A proof of the replica formula},
  author={Dia, Mohamad and Macris, Nicolas and Krzakala, Florent and Lesieur, Thibault and Zdeborov{\'a}, Lenka and others},
  booktitle={Advances in Neural Information Processing Systems},
  pages={424--432},
  year={2016}
}

@inproceedings{aubin2018committee,
  title={The committee machine: Computational to statistical gaps in learning a two-layers neural network},
  author={Aubin, Benjamin and Maillard, Antoine and Krzakala, Florent and Macris, Nicolas and Zdeborov{\'a}, Lenka and others},
  booktitle={Advances in Neural Information Processing Systems},
  pages={3223--3234},
  year={2018}
}


@inproceedings{malzahn2003statistical,
  title={A statistical mechanics approach to approximate analytical bootstrap averages},
  author={Malzahn, D{\"o}rthe and Opper, Manfred},
  booktitle={Advances in Neural Information Processing Systems},
  pages={343--350},
  year={2003}
}

@inproceedings{rangan2009asymptotic,
  title={Asymptotic analysis of map estimation via the replica method and compressed sensing},
  author={Rangan, Sundeep and Goyal, Vivek and Fletcher, Alyson K},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1545--1553},
  year={2009}
}

% Gardner
@inproceedings{paszke2019pytorch,
  title={PyTorch: An imperative style, high-performance deep learning library},
  author={Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and others},
  booktitle={Advances in Neural Information Processing Systems},
  pages={8024--8035},
  year={2019}
}


@book{engel-vandenbroek,
  title={Statistical mechanics of learning},
  author={Engel, Andreas and Van den Broeck, Christian},
  year={2001},
  publisher={Cambridge University Press}}
}
@article{huang2014origin,
  title={Origin of the computational hardness for learning with binary synapses},
  author={Huang, Haiping and Kabashima, Yoshiyuki},
  journal={Physical Review E},
  volume={90},
  number={5},
  pages={052813},
  year={2014},
  publisher={American Physical Society}
}

@article{monasson1995structural,
  title={Structural glass transition and the entropy of the metastable states},
  author={Monasson, R{\'e}mi},
  journal={Physical review letters},
  volume={75},
  number={15},
  pages={2847},
  year={1995},
  publisher={APS}
}



% Gaussian Mixtures

% gaussian mixtures

@article{mai2019high,
  title={High Dimensional Classification via Empirical Risk Minimization: Improvements and Optimality},
  author={Mai, Xiaoyi and Liao, Zhenyu},
  journal={arXiv preprint arXiv:1905.13742},
  year={2019}
}

@article{Lelarge2019,
  title={Asymptotic Bayes risk for Gaussian mixture in a semi-supervised setting},
  author={Lelarge, Marc and Miolane, Leo},
  journal={arXiv preprint arXiv:1907.03792},
  year={2019}
}

@article{deng2019model,
  title={A Model of Double Descent for High-dimensional Binary Linear Classification},
  author={Deng, Zeyu and Kammoun, Abla and Thrampoulidis, Christos},
  journal={arXiv preprint arXiv:1911.05822},
  year={2019}
}


@inproceedings{Lesieur2016,
  title={Phase transitions and optimal algorithms in high-dimensional Gaussian mixture clustering},
  author={Lesieur, Thibault and De Bacco, Caterina and Banks, Jess and Krzakala, Florent and Moore, Cris and Zdeborov{\'a}, Lenka},
  booktitle={2016 54th Annual Allerton Conference on Communication, Control, and Computing (Allerton)},
  pages={601--608},
  year={2016},
  organization={IEEE}
}


@article{Mignacco2020,
	title={The role of regularization in classification of high-dimensional noisy Gaussian mixture},
	author={Mignacco, Francesca and Krzakala, Florent and Lu, Yue M and Zdeborov{\'a}, Lenka},
	journal={arXiv preprint arXiv:2002.11544},
	year={2020}
}


% network models

% LeNet
@ARTICLE{lenet,
author={Y. {Lecun} and L. {Bottou} and Y. {Bengio} and P. {Haffner}},
journal={Proceedings of the IEEE},
title={Gradient-based learning applied to document recognition},
year={1998},
volume={86},
number={11},
pages={2278-2324},
doi={10.1109/5.726791},
ISSN={1558-2256},
month={Nov},}

% ResNet
@INPROCEEDINGS{resnet,
author={K. {He} and X. {Zhang} and S. {Ren} and J. {Sun}},
booktitle={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
title={Deep Residual Learning for Image Recognition},
year={2016},
volume={},
number={},
pages={770-778},
doi={10.1109/CVPR.2016.90},
ISSN={1063-6919},
month={June},}

@inproceedings{densenet,
  title={Densely connected convolutional networks},
  author={Huang, Gao and Liu, Zhuang and Van Der Maaten, Laurens and Weinberger, Kilian Q},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={4700--4708},
  year={2017}
}

@article{chaudhari2018deep,
  title={Deep relaxation: partial differential equations for optimizing deep neural networks},
  author={Chaudhari, Pratik and Oberman, Adam and Osher, Stanley and Soatto, Stefano and Carlier, Guillaume},
  journal={Research in the Mathematical Sciences},
  volume={5},
  number={3},
  pages={30},
  year={2018},
  publisher={Springer}
}

% EfficientNet (should find it on ICML2019)
@misc{efficientnet,
    title={EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks},
    author={Mingxing Tan and Quoc V. Le},
    year={2019},
    eprint={1905.11946},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

% EfficientNet bis
@misc{efficientnetbis,
    title={Adversarial Examples Improve Image Recognition},
    author={Cihang Xie and Mingxing Tan and Boqing Gong and Jiang Wang and Alan Yuille and Quoc V. Le},
    year={2019},
    eprint={1911.09665},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}

% efficientnet with noisy student
@misc{efficientnetnoisystudent,
    title={Self-training with Noisy Student improves ImageNet classification},
    author={Qizhe Xie and Eduard Hovy and Minh-Thang Luong and Quoc V. Le},
    year={2019},
    eprint={1911.04252},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@article{buntine1991bayesian,
author = {Buntine, Wray L and Weigend, Andreas S},
journal = {Complex systems},
number = {6},
pages = {603--643},
title = {{Bayesian back-propagation}},
volume = {5},
year = {1991}
}


% ResNext
@InProceedings{resnext,
author="Mahajan, Dhruv and Girshick, Ross and Ramanathan, Vignesh and He, Kaiming and Paluri, Manohar and Li, Yixuan and Bharambe, Ashwin and van der Maaten, Laurens",
editor="Ferrari, Vittorio and Hebert, Martial and Sminchisescu, Cristian and Weiss, Yair",
title="Exploring the Limits of Weakly Supervised Pretraining",
booktitle="Computer Vision -- ECCV 2018",
year="2018",
publisher="Springer International Publishing",
address="Cham",
pages="185--201",
isbn="978-3-030-01216-8"
}

% FixResNeXt
@incollection{fixresnext,
title = {Fixing the train-test resolution discrepancy},
author = {Touvron, Hugo and Vedaldi, Andrea and Douze, Matthijs and Jegou, Herve},
booktitle = {Advances in Neural Information Processing Systems 32},
editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d Alch\'{e}-Buc and E. Fox and R. Garnett},
pages = {8250--8260},
year = {2019},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/9035-fixing-the-train-test-resolution-discrepancy.pdf}
}

@inproceedings{Hinton1993,
author = {Hinton, Geoffrey E. and van Camp, Drew},
title = {Keeping the Neural Networks Simple by Minimizing the Description Length of the Weights},
year = {1993},
isbn = {0897916115},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/168304.168306},
doi = {10.1145/168304.168306},
booktitle = {Proceedings of the Sixth Annual Conference on Computational Learning Theory},
pages = {5–13},
numpages = {9},
location = {Santa Cruz, California, USA},
series = {COLT ’93}
}



@article{pyramidnet,
  author    = {Dongyoon Han and
               Jiwhan Kim and
               Junmo Kim},
  title     = {Deep Pyramidal Residual Networks},
  journal   = {CoRR},
  volume    = {abs/1610.02915},
  year      = {2016},
  url       = {http://arxiv.org/abs/1610.02915},
  archivePrefix = {arXiv},
  eprint    = {1610.02915},
  timestamp = {Mon, 13 Aug 2018 16:47:11 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/HanKK16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{shakedrop,
  author    = {Yoshihiro Yamada and
               Masakazu Iwamura and
               Koichi Kise},
  title     = {ShakeDrop regularization},
  journal   = {CoRR},
  volume    = {abs/1802.02375},
  year      = {2018},
  url       = {http://arxiv.org/abs/1802.02375},
  archivePrefix = {arXiv},
  eprint    = {1802.02375},
  timestamp = {Mon, 13 Aug 2018 16:46:25 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1802-02375.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{AA,
  author    = {Ekin Dogus Cubuk and
               Barret Zoph and
               Dandelion Man{\'{e}} and
               Vijay Vasudevan and
               Quoc V. Le},
  title     = {AutoAugment: Learning Augmentation Policies from Data},
  journal   = {CoRR},
  volume    = {abs/1805.09501},
  year      = {2018},
  url       = {http://arxiv.org/abs/1805.09501},
  archivePrefix = {arXiv},
  eprint    = {1805.09501},
  timestamp = {Mon, 13 Aug 2018 16:48:44 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1805-09501.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{fastAA,
  author    = {Sungbin Lim and
               Ildoo Kim and
               Taesup Kim and
               Chiheon Kim and
               Sungwoong Kim},
  title     = {Fast AutoAugment},
  journal   = {CoRR},
  volume    = {abs/1905.00397},
  year      = {2019},
  url       = {http://arxiv.org/abs/1905.00397},
  archivePrefix = {arXiv},
  eprint    = {1905.00397},
  timestamp = {Mon, 27 May 2019 13:15:00 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1905-00397.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{cutout,
  author    = {Terrance Devries and
               Graham W. Taylor},
  title     = {Improved Regularization of Convolutional Neural Networks with Cutout},
  journal   = {CoRR},
  volume    = {abs/1708.04552},
  year      = {2017},
  archivePrefix = {arXiv},
  eprint    = {1708.04552},
  timestamp = {Mon, 13 Aug 2018 16:45:57 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1708-04552.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
% generalization measures

@article{hochreiter,
author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
title = {Flat Minima},
journal = {Neural Computation},
volume = {9},
number = {1},
pages = {1-42},
year = {1997},
doi = {10.1162/neco.1997.9.1.1},
URL = {https://doi.org/10.1162/neco.1997.9.1.1},
eprint = {https://doi.org/10.1162/neco.1997.9.1.1}
}

@misc{jiang2019fantastic,
    title={Fantastic Generalization Measures and Where to Find Them},
    author={Yiding Jiang and Behnam Neyshabur and Hossein Mobahi and Dilip Krishnan and Samy Bengio},
    year={2019},
    eprint={1912.02178},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@article{Dinh2017,
abstract = {Despite their overwhelming capacity to ovcrfit, deep learning architectures tend to generalize relatively well to unseen data, allowing them to be deployed in practice. However, explaining why this is the case is still an open area of research. One standing hypothesis that is gaining popularity, e.g. Hochreiter {\&} Schmidhuber (1997); Keskar et al. (2017), is that the flatness of minima of the loss function found by stochastic gradient based methods results in good generalization. This paper argues that most notions of flatness are problematic for deep models and can not be directly applied to explain generalization. Specifically, when focusing on deep networks with rectifier units, we can exploit the particular geometry of parameter space induced by the inherent symmetries that these architectures exhibit to build equivalent models corresponding to arbitrarily sharper minima. Furthermore, if we allow to reparamelrize a function, the geometry of its parameters can change drastically without affecting its generalization properties.},
archivePrefix = {arXiv},
arxivId = {1703.04933},
author = {Dinh, Laurent and Pascanu, Razvan and Bengio, Samy and Bengio, Yoshua},
eprint = {1703.04933},

isbn = {9781510855144},
journal = {34th International Conference on Machine Learning, ICML 2017},
pages = {1705--1714},
title = {{Sharp minima can generalize for deep nets}},
volume = {3},
year = {2017}
}


@article{keskar,
  author    = {Nitish Shirish Keskar and
               Dheevatsa Mudigere and
               Jorge Nocedal and
               Mikhail Smelyanskiy and
               Ping Tak Peter Tang},
  title     = {On Large-Batch Training for Deep Learning: Generalization Gap and
               Sharp Minima},
  journal   = {CoRR},
  volume    = {abs/1609.04836},
  year      = {2016},
  url       = {http://arxiv.org/abs/1609.04836},
  archivePrefix = {arXiv},
  eprint    = {1609.04836},
  timestamp = {Mon, 13 Aug 2018 16:46:48 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/KeskarMNST16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{orbanz,
    title={Non-Vacuous Generalization Bounds at the ImageNet Scale: A PAC-Bayesian Compression Approach},
    author={Wenda Zhou and Victor Veitch and Morgane Austern and Ryan P. Adams and Peter Orbanz},
    year={2018},
    eprint={1804.05862},
    archivePrefix={arXiv},
    primaryClass={stat.ML}
}

@misc{dziugaite1,
    title={Computing Nonvacuous Generalization Bounds for Deep (Stochastic) Neural Networks with Many More Parameters than Training Data},
    author={Gintare Karolina Dziugaite and Daniel M. Roy},
    year={2017},
    eprint={1703.11008},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@misc{
dziugaite2,
title={Entropy-{SGD} optimizes the prior of a {PAC}-Bayes bound: Data-dependent {PAC}-Bayes priors via differential privacy},
author={Gintare Karolina Dziugaite and Daniel M. Roy},
year={2018},
url={https://openreview.net/forum?id=ry9tUX_6-},
}

@misc{elastic,
    title={Deep learning with Elastic Averaging SGD},
    author={Sixin Zhang and Anna Choromanska and Yann LeCun},
    year={2014},
    eprint={1412.6651},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@inproceedings{welling2011bayesian,
  title={Bayesian learning via stochastic gradient Langevin dynamics},
  author={Welling, Max and Teh, Yee W},
  booktitle={Proceedings of the 28th international conference on machine learning (ICML-11)},
  pages={681--688},
  year={2011}
}


@article{ju2018relative,
  title={The relative performance of ensemble methods with deep convolutional neural networks for image classification},
  author={Ju, Cheng and Bibaut, Aur{\'e}lien and van der Laan, Mark},
  journal={Journal of Applied Statistics},
  volume={45},
  number={15},
  pages={2800--2818},
  year={2018},
  publisher={Taylor \& Francis}
}

@article{rosen1996ensemble,
  title={Ensemble learning using decorrelated neural networks},
  author={Rosen, Bruce E},
  journal={Connection science},
  volume={8},
  number={3-4},
  pages={373--384},
  year={1996},
  publisher={Taylor \& Francis}
}


@article{hansen1990neural,
  title={Neural network ensembles},
  author={Hansen, Lars Kai and Salamon, Peter},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={12},
  number={10},
  pages={993--1001},
  year={1990},
  publisher={IEEE}
}

@article{lecun1998gradient,
  title={Gradient-based learning applied to document recognition},
  author={LeCun, Yann and Bottou, L{\'e}on and Bengio, Yoshua and Haffner, Patrick},
  journal={Proceedings of the IEEE},
  volume={86},
  number={11},
  pages={2278--2324},
  year={1998},
  publisher={Ieee}
}


% optimization algorithms
@inproceedings{entropysgdICLR,
  author    = {Pratik Chaudhari and
               Anna Choromanska and
               Stefano Soatto and
               Yann LeCun and
               Carlo Baldassi and
               Christian Borgs and
               Jennifer T. Chayes and
               Levent Sagun and
               Riccardo Zecchina},
  title     = {Entropy-SGD: Biasing Gradient Descent Into Wide Valleys},
  booktitle = {5th International Conference on Learning Representations, {ICLR} 2017,
               Toulon, France, April 24-26, 2017, Conference Track Proceedings},
  publisher = {OpenReview.net},
  year      = {2017},
  url       = {https://openreview.net/forum?id=B1YfAfcgl},
  timestamp = {Thu, 25 Jul 2019 14:26:02 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/ChaudhariCSLBBC17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{entropysgd,
  title={Entropy-sgd: Biasing gradient descent into wide valleys},
  author={Chaudhari, Pratik and Choromanska, Anna and Soatto, Stefano and LeCun, Yann and Baldassi, Carlo and Borgs, Christian and Chayes, Jennifer and Sagun, Levent and Zecchina, Riccardo},
  journal={Journal of Statistical Mechanics: Theory and Experiment},
  volume={2019},
  number={12},
  pages={124018},
  year={2019},
  publisher={IOP Publishing}
}

@article{parle,
  author    = {Pratik Chaudhari and
               Carlo Baldassi and
               Riccardo Zecchina and
               Stefano Soatto and
               Ameet Talwalkar},
  title     = {Parle: parallelizing stochastic gradient descent},
  journal   = {CoRR},
  volume    = {abs/1707.00424},
  year      = {2017},
  url       = {http://arxiv.org/abs/1707.00424},
  archivePrefix = {arXiv},
  eprint    = {1707.00424},
  timestamp = {Mon, 13 Aug 2018 16:47:16 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/ChaudhariBZST17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

%local entropy papers

@article{locentfirst,
  title = {Subdominant Dense Clusters Allow for Simple Learning and High Computational Performance in Neural Networks with Discrete Synapses},
  author = {Baldassi, Carlo and Ingrosso, Alessandro and Lucibello, Carlo and Saglietti, Luca and Zecchina, Riccardo},
  journal = {Phys. Rev. Lett.},
  volume = {115},
  issue = {12},
  pages = {128101},
  numpages = {5},
  year = {2015},
  month = {Sep},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevLett.115.128101},
  url = {https://link.aps.org/doi/10.1103/PhysRevLett.115.128101}
}
@article {unreasoanable,
	author = {Baldassi, Carlo and Borgs, Christian and Chayes, Jennifer T. and Ingrosso, Alessandro and Lucibello, Carlo and Saglietti, Luca and Zecchina, Riccardo},
	title = {Unreasonable effectiveness of learning neural networks: From accessible states and robust ensembles to basic algorithmic schemes},
	volume = {113},
	number = {48},
	pages = {E7655--E7662},
	year = {2016},
	doi = {10.1073/pnas.1608103113},
	publisher = {National Academy of Sciences},
	issn = {0027-8424},
	URL = {https://www.pnas.org/content/113/48/E7655},
	eprint = {https://www.pnas.org/content/113/48/E7655.full.pdf},
	journal = {Proceedings of the National Academy of Sciences}
}

@article {baldassi2019shaping,
	author = {Baldassi, Carlo and Pittorino, Fabrizio and Zecchina, Riccardo},
	title = {Shaping the learning landscape in neural networks around wide flat minima},
	volume = {117},
	number = {1},
	pages = {161--170},
	year = {2020},
	doi = {10.1073/pnas.1908636117},
	publisher = {National Academy of Sciences},
	issn = {0027-8424},
	URL = {https://www.pnas.org/content/117/1/161},
	eprint = {https://www.pnas.org/content/117/1/161.full.pdf},
	journal = {Proceedings of the National Academy of Sciences}
}

@article{relu_locent,
  title = {Properties of the Geometry of Solutions and Capacity of Multilayer Neural Networks with Rectified Linear Unit Activations},
  author = {Baldassi, Carlo and Malatesta, Enrico M. and Zecchina, Riccardo},
  journal = {Phys. Rev. Lett.},
  volume = {123},
  issue = {17},
  pages = {170602},
  numpages = {6},
  year = {2019},
  month = {Oct},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevLett.123.170602},
  url = {https://link.aps.org/doi/10.1103/PhysRevLett.123.170602}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
@inproceedings{kour2014real,
  title={Real-time segmentation of on-line handwritten arabic script},
  author={Kour, George and Saabne, Raid},
  booktitle={Frontiers in Handwriting Recognition (ICFHR), 2014 14th International Conference on},
  pages={417--422},
  year={2014},
  organization={IEEE}
}

@inproceedings{kour2014fast,
  title={Fast classification of handwritten on-line Arabic characters},
  author={Kour, George and Saabne, Raid},
  booktitle={Soft Computing and Pattern Recognition (SoCPaR), 2014 6th International Conference of},
  pages={312--318},
  year={2014},
  organization={IEEE}
}

@article{hadash2018estimate,
  title={Estimate and Replace: A Novel Approach to Integrating Deep Neural Networks with Existing Applications},
  author={Hadash, Guy and Kermany, Einat and Carmeli, Boaz and Lavi, Ofer and Kour, George and Jacovi, Alon},
  journal={arXiv preprint arXiv:1804.09028},
  year={2018}
}

% Adam (should find it at ICLR)
@misc{kingma2014adam,
    title={Adam: A Method for Stochastic Optimization},
    author={Diederik P. Kingma and Jimmy Ba},
    year={2014},
    eprint={1412.6980},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@incollection{sgdvsadam,
title = {The Marginal Value of Adaptive Gradient Methods in Machine Learning},
author = {Wilson, Ashia C and Roelofs, Rebecca and Stern, Mitchell and Srebro, Nati and Recht, Benjamin},
booktitle = {Advances in Neural Information Processing Systems 30},
editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
pages = {4148--4158},
year = {2017},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/7003-the-marginal-value-of-adaptive-gradient-methods-in-machine-learning.pdf}
}

@article{baldassi_local_2016,
  title = {Local entropy as a measure for sampling solutions in constraint satisfaction problems},
  volume = {2016},
  issn = {1742-5468},
  url = {http://stacks.iop.org/1742-5468/2016/i=2/a=023301?key=crossref.a72a5bd1abacd77b91afb369eff15a65},
  doi = {10.1088/1742-5468/2016/02/023301},
  number = {2},
  urldate = {2016-02-25},
  journal = {Journal of Statistical Mechanics: Theory and Experiment},
  author = {Baldassi, Carlo and Ingrosso, Alessandro and Lucibello, Carlo and Saglietti, Luca and Zecchina, Riccardo},
  month = feb,
  year = {2016},
  pages = {P023301},
}

@inproceedings{zhang2019lookahead,
  title={Lookahead Optimizer: k steps forward, 1 step back},
  author={Zhang, Michael and Lucas, James and Ba, Jimmy and Hinton, Geoffrey E},
  booktitle={Advances in Neural Information Processing Systems},
  pages={9593--9604},
  year={2019}
}


@inproceedings{mannelli2019afraid,
  title={Who is afraid of big bad minima? analysis of gradient-flow in spiked matrix-tensor models},
  author={Mannelli, Stefano Sarao and Biroli, Giulio and Cammarota, Chiara and Krzakala, Florent and Zdeborov{\'a}, Lenka},
  booktitle={Advances in Neural Information Processing Systems},
  pages={8679--8689},
  year={2019}
}

@inproceedings{kirkpatrick1994statistical,
  title={The statistical mechanics of k-satisfaction},
  author={Kirkpatrick, Scott and Gyorgyi, Geza and Tishby, Naftali and Troyansky, Lidror},
  booktitle={Advances in Neural Information Processing Systems},
  pages={439--446},
  year={1994}
}

@inproceedings{malzahn2002variational,
  title={A variational approach to learning curves},
  author={Malzahn, D{\"o}rthe and Opper, Manfred},
  booktitle={Advances in neural information processing systems},
  pages={463--469},
  year={2002}
}


@article{Barbier2019,
	doi = {10.1088/1751-8121/ab2735},
	year = 2019,
	month = {jun},
	publisher = {{IOP} Publishing},
	volume = {52},
	number = {29},
	pages = {294002},
	author = {Jean Barbier and Nicolas Macris},
	title = {The adaptive interpolation method for proving replica formulas. Applications to the Curie{\textendash}Weiss and Wigner spike models},
	journal = {Journal of Physics A: Mathematical and Theoretical},
	abstract = {In this contribution we give a pedagogic introduction to the newly introduced adaptive interpolation method to prove in a simple and unified way replica formulas for Bayesian optimal inference problems. Many aspects of this method can already be explained at the level of the simple Curie–Weiss spin system. This provides a new method of solution for this model which does not appear to be known. We then generalize this analysis to a paradigmatic inference problem, namely rank-one matrix estimation, also refered to as the Wigner spike model in statistics. We give many pointers to the recent literature where the method has been succesfully applied.}
 }
 
 
% Castellani Cavagna
@article{CastellaniCavagna2005,
	doi = {10.1088/1742-5468/2005/05/p05012},
	year = 2005,
	month = {may},
	publisher = {{IOP} Publishing},
	volume = {2005},
	number = {05},
	pages = {P05012},
	author = {Tommaso Castellani and Andrea Cavagna},
	title = {Spin-glass theory for pedestrians},
	journal = {Journal of Statistical Mechanics: Theory and Experiment},
	abstract = {In these notes the main theoretical concepts and techniques in the field of mean-field spin
glasses are reviewed in a compact and pedagogical way, for the benefit of the graduate
and undergraduate student. One particular spin-glass model is analysed (the
p-spin spherical model) by using three different approaches: thermodynamics, covering
pure states, overlaps, overlap distribution, replica symmetry breaking, and the
static transition; dynamics, covering the generating functional method, generalized
Langevin equation, equations for the correlation and the response, the mode
coupling approximation, and the dynamical transition; and finally complexity,
covering the mean-field (Thouless–Anderson–Palmer) free energy, metastable states,
entropy crisis, threshold energy, and saddles. Particular attention has been paid
to the mutual consistency of the results obtained from the different methods.}
}


@article{gardner1988The,
	doi = {10.1088/0305-4470/21/1/030},
	year = 1988,
	month = {jan},
	publisher = {{IOP} Publishing},
	volume = {21},
	number = {1},
	pages = {257--270},
	author = {E Gardner},
	title = {The space of interactions in neural network models},
	journal = {Journal of Physics A: Mathematical and General},
	abstract = {The typical fraction of the space of interactions between each pair of N Ising spins which solve the problem of storing a given set of p random patterns as N-bit spin configurations is considered. The volume is calculated explicitly as a function of the storage ratio, alpha =p/N, of the value kappa (>0) of the product of the spin and the magnetic field at each site and of the magnetisation, m. Here m may vary between 0 (no correlation) and 1 (completely correlated). The capacity increases with the correlation between patterns from alpha =2 for correlated patterns with kappa =0 and tends to infinity as m tends to 1. The calculations use a saddle-point method and the order parameters at the saddle point are assumed to be replica symmetric. This solution is shown to be locally stable. A local iterative learning algorithm for updating the interactions is given which will converge to a solution of given kappa provided such solutions exist.}
}

@article{gardner1988optimal,
	doi = {10.1088/0305-4470/21/1/031},
	year = 1988,
	month = {jan},
	publisher = {{IOP} Publishing},
	volume = {21},
	number = {1},
	pages = {271--284},
	author = {E Gardner and B Derrida},
	title = {Optimal storage properties of neural network models},
	journal = {Journal of Physics A: Mathematical and General},
	abstract = {The authors calculate the number, p= alpha N of random N-bit patterns that an optimal neural network can store allowing a given fraction f of bit errors and with the condition that each right bit is stabilised by a local field at least equal to a parameter K. For each value of alpha and K, there is a minimum fraction fmin of wrong bits. They find a critical line, alpha c(K) with alpha c(0)=2. The minimum fraction of wrong bits vanishes for alpha  alpha c(K). The calculations are done using a saddle-point method and the order parameters at the saddle point are assumed to be replica symmetric. This solution is locally stable in a finite region of the K, alpha plane including the line, alpha c(K) but there is a line above which the solution becomes unstable and replica symmetry must be broken.}
}


@book{mezard1987spin,
  title={Spin glass theory and beyond: An Introduction to the Replica Method and Its Applications},
  author={M{\'e}zard, Marc and Parisi, Giorgio and Virasoro, Miguel},
  volume={9},
  year={1987},
  publisher={World Scientific Publishing Company}
}

@article{franz1995recipes,
  title={Recipes for metastable states in spin glasses},
  author={Franz, Silvio and Parisi, Giorgio},
  doi={10.1051/jp1:1995201},
  journal={Journal de Physique I},
  volume={5},
  number={11},
  pages={1401--1415},
  year={1995},
  publisher={EDP Sciences}
}

@book{mezard2009information,
  title={Information, physics, and computation},
  author={Mezard, Marc and Montanari, Andrea},
  year={2009},
  publisher={Oxford University Press}
}

@inproceedings{salimans2016weight,
  title={Weight normalization: A simple reparameterization to accelerate training of deep neural networks},
  author={Salimans, Tim and Kingma, Durk P},
  booktitle={Advances in neural information processing systems},
  pages={901--909},
  year={2016}
}

