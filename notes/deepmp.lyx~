#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\begin_preamble
\DeclareMathOperator\atanh{atanh}
\DeclareMathOperator\sign{sign}
\end_preamble
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Deep Message Passing
\end_layout

\begin_layout Section
Problem Setting
\end_layout

\begin_layout Standard
TODO ENTIRE SECTION.
\end_layout

\begin_layout Standard
In this deep inference problem, we assume that a signal with prior 
\begin_inset Formula $P^{\text{in}}$
\end_inset

 is fed to a deep feedforward networks with 
\begin_inset Formula $L+1$
\end_inset

 layers of weights 
\begin_inset Formula $\boldsymbol{W}^{\ell}\in\mathbb{R}^{N_{\ell+1}\times N_{\ell}},\ell=0,\dots,L$
\end_inset

 and biases 
\begin_inset Formula $\boldsymbol{b}^{\ell}\in\text{\ensuremath{\mathbb{R}^{N_{\ell+1}}}}$
\end_inset

.
 The signal is propagated through stochastic neuron layers described by
 probability distributions 
\begin_inset Formula $P^{\ell}$
\end_inset

 conditioned on the preactivations, therefore we have the following Markov
 chain:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align}
\boldsymbol{x}^{0} & \sim P^{\text{in}}\\
\boldsymbol{x}^{\ell+1} & \sim P^{\ell+1}\left(\bullet\,|\,\boldsymbol{W}^{\ell}\boldsymbol{x}^{\ell}+\boldsymbol{b}^{\ell}\right)\qquad\ell=0,\dots,L
\end{align}

\end_inset


\end_layout

\begin_layout Standard
Only 
\begin_inset Formula $\boldsymbol{y}=\boldsymbol{x}^{L+1}$
\end_inset

 is observed, and the task is to reconstruct the original signal 
\begin_inset Formula $\boldsymbol{x}^{0}$
\end_inset

.
 The posterior distribution 
\begin_inset Formula $p(\boldsymbol{x}^{0:L})=P(\boldsymbol{x}^{0:L}\,|\,\boldsymbol{x}^{L+1}=\boldsymbol{y})$
\end_inset

 reads
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align}
p(\boldsymbol{x}^{0:L}) & \propto\prod_{\ell=0}^{L}\prod_{k=1}^{N_{\ell+1}}\,P_{k}^{\ell+1}\left(x_{k}^{\ell+1}\ \bigg|\ \sum_{i=1}^{N_{\ell}}W_{ki}^{\ell}x_{i}^{\ell}\right)\ \prod_{k=1}^{N_{0}}P_{k}^{\text{in}}(x_{k}^{0}),
\end{align}

\end_inset


\end_layout

\begin_layout Standard
Typical channels are given by deterministic elementwise activation function
 
\begin_inset Formula $f_{\ell}(z)$
\end_inset

 (e.g.
 
\begin_inset Formula $f_{\ell}(z)=\sign(x)$
\end_inset

 or 
\begin_inset Formula $f_{\ell}(z)=\text{relu}(z)=\max(0,z)$
\end_inset

), combined with Gaussian additive pre-activation noise with variance 
\begin_inset Formula $\sigma^{2}$
\end_inset

.
 In such cases we have
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
P_{k}^{\ell}(x\,|\,z)=\int D\xi\,\delta\left(x-f_{\ell}(z+\sigma_{k}^{\ell}\,\xi)\right)
\]

\end_inset


\end_layout

\begin_layout Standard
We also call 
\begin_inset Formula $\alpha_{\ell}=N_{\ell+1}/N_{\ell}$
\end_inset

 the layer 
\emph on
expansion ratio.
\end_layout

\begin_layout Section
Deep Belief Propagation
\end_layout

\begin_layout Standard
The AMP equations have been derived from the BP equation in the Appendix.
 First we introduce the neuron scalar entropy functions:
\begin_inset Formula 
\begin{align}
\varphi_{k}^{0}(B,A) & =\log\int\mathrm{d}x\ e^{-\frac{1}{2}A^{2}x^{2}+Bx}\,P_{k}^{\text{in}}(x)\\
\varphi_{k}^{\ell}(B,A,\omega,V) & =\log\int\mathrm{d}x\,\mathrm{d}z\ e^{-\frac{1}{2}A^{2}x^{2}+Bx}\,P_{k}^{\ell}\left(x|z\right)e^{-\frac{(\omega-z)^{2}}{2V}}\qquad\ell=1,\dots,L\\
\varphi_{k}^{L+1}(\omega,V,y) & =\log\int\mathrm{d}z\ P_{k}^{L+1}\left(y|z\right)e^{-\frac{(\omega-z)^{2}}{2V}}\\
\psi_{ki}^{\ell}(H,G) & =\log\int\mathrm{d}w\ e^{-\frac{1}{2}G^{2}w^{2}+Hw}\,P_{ki}^{\text{\ensuremath{\ell}}}(w)
\end{align}

\end_inset


\end_layout

\begin_layout Standard
For convenience define 
\begin_inset Formula $\varphi_{i}^{0,t}=\varphi_{i}^{0}\left(B_{i}^{0,t},A_{i}^{0,t}\right)$
\end_inset

 and 
\begin_inset Formula $\varphi_{i}^{\ell,t}=\varphi_{i}^{\ell}\left(B_{i}^{\ell,t},A_{i}^{\ell,t},\omega_{i}^{\ell-1,t},V_{i}^{\ell-1,t}\right)$
\end_inset

 and 
\begin_inset Formula $\varphi_{i}^{L+1,t}=\varphi_{i}^{L+1}\left(\omega_{i}^{L,t},V_{i}^{L,t},y_{i}\right)$
\end_inset

.
 
\end_layout

\begin_layout Standard
Then, we can decompose the BP update rules in a forward and a backward step.
\end_layout

\begin_layout Paragraph
Forward pass.
\end_layout

\begin_layout Standard
As the initial condition for the iterations, we set to zero the following
 quantities: 
\begin_inset Formula $B_{i}^{\ell,t=0}=0,A_{i}^{\ell,t=0}=0$
\end_inset

 and 
\begin_inset Formula $g_{k}^{\ell,t=0}=0$
\end_inset

.
 The following iterations hold at time 
\begin_inset Formula $t\geq1$
\end_inset

.
 In the FORWARD pass, starting from 
\begin_inset Formula $\ell=0$
\end_inset

 and up to 
\begin_inset Formula $\ell=L$
\end_inset

, we have
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align}
\hat{x}_{ia\to k}^{\ell,t} & =\partial_{B}\varphi_{ia\to k}^{\ell}\left(B_{ia\to k}^{\ell,t-1},A_{ia}^{\ell,t-1},\omega_{ia}^{\ell-1,t},V_{ia}^{\ell-1,t}\right)\\
\Delta_{ia\to k}^{\ell+1,t} & =\partial_{B}^{2}\varphi_{ia\to k}^{\ell+1,t}\\
m_{ki\to a}^{\ell,t} & =\partial_{H}\psi_{ki}^{\ell}(H_{ki\to a}^{t-1},G_{ki}^{t-1})\\
\sigma_{ki\to a}^{\ell,t} & =\partial_{H}^{2}\psi_{ki}^{\ell}(H_{ki\to a}^{t-1},G_{ki}^{t-1})\\
V_{ka}^{\ell,t} & =\sum_{i}\left(\left(m_{ki\to a}^{\ell,t}\right)^{2}\Delta_{ia\to k}^{\ell,t}+\Sigma_{ki\to a}^{\ell,t}(\hat{x}_{ia\to k}^{\ell,t})^{2}+\sigma_{ki\to a}^{\ell,t}\Delta_{ia\to k}^{\ell,t}\right)\label{eq:upd-V-2}\\
\omega_{ka\to i}^{\ell,t} & =\sum_{i'\neq i}m_{ki\to a}^{\ell,t}\hat{x}_{ia\to k}^{\ell,t}\label{eq:upd-w-2}
\end{align}

\end_inset


\end_layout

\begin_layout Standard
Here 
\begin_inset Formula $V^{\ell}$
\end_inset

 and 
\begin_inset Formula $\omega^{\ell}$
\end_inset

 are computed as a function of the previous layer values 
\begin_inset Formula $V^{\ell-1}$
\end_inset

 and 
\begin_inset Formula $\omega^{\ell-1}$
\end_inset

.
 
\end_layout

\begin_layout Paragraph
Backward pass.
\end_layout

\begin_layout Standard
In the BACKWARD sweep, starting from 
\begin_inset Formula $\ell=L$
\end_inset

 and down to 
\begin_inset Formula $\ell=0$
\end_inset

, we have 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align}
g_{ka\to i}^{\ell,t} & =\partial_{\omega}\varphi_{ka\to i}^{\ell+1}\left(B_{ka}^{\ell+1,t},A_{ka}^{\ell+1,t},\omega_{ka\to i}^{\ell,t},V_{ka}^{\ell,t}\right)\label{eq:upd-g-2}\\
\Gamma_{ka\to i}^{\ell,t} & =-\partial_{\omega}^{2}\varphi_{ka\to i}^{\ell+1,t}\label{eq:upd-dwg-2}\\
A_{ia}^{\ell,t} & =\sum_{k}\left((m_{ki\to a}^{\ell,t})^{2}+\sigma_{ki\to a}^{\ell,t}\right)\Gamma_{ka\to i}^{\ell,t}-\sigma_{ki\to a}^{\ell,t}\left(g_{ka\to i}^{\ell,t}\right)^{2}\label{eq:upd-A-2}\\
B_{ia}^{\ell,t} & =\sum_{k}m_{ki\to a}^{\ell}g_{ka\to i}^{\ell,t}\label{eq:upd-B-2}\\
G_{ki}^{\ell,t} & =\sum_{a}\left((\hat{x}_{ia\to k}^{\ell,t})^{2}+\Delta_{ia\to k}\right)\Gamma_{ka\to i}^{\ell,t}-\Delta_{ia\to k}\left(g_{ka\to i}^{\ell,t}\right)^{2}\\
H_{ki\to a} & =\sum_{a'\neq a}\hat{x}_{ia'\to k}^{\ell,t}g_{ka'\to i}^{\ell,t}
\end{align}

\end_inset


\end_layout

\begin_layout Standard
Notice that 
\begin_inset Formula $A^{\ell}$
\end_inset

 and 
\begin_inset Formula $B^{\ell}$
\end_inset

 are computed as as a function of the 
\begin_inset Formula $A^{\ell+1},B^{\ell+1}$
\end_inset

 of the layer above, with the initial condition given by the output 
\begin_inset Formula $\boldsymbol{x}^{L+1}=\boldsymbol{y}$
\end_inset

 on the top layer.
\end_layout

\begin_layout Section
Deep AMP
\end_layout

\begin_layout Paragraph
Forward pass.
\end_layout

\begin_layout Standard
As the initial condition for the iterations, we set to zero the following
 quantities: 
\begin_inset Formula $B_{i}^{\ell,t=0}=0,A_{i}^{\ell,t=0}=0$
\end_inset

 and 
\begin_inset Formula $g_{k}^{\ell,t=0}=0$
\end_inset

.
 The following iterations hold at time 
\begin_inset Formula $t\geq1$
\end_inset

.
 In the FORWARD pass, starting from 
\begin_inset Formula $\ell=0$
\end_inset

 and up to 
\begin_inset Formula $\ell=L$
\end_inset

, we have
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align}
\hat{x}_{ia}^{\ell,t} & =\partial_{B}\varphi_{ia}^{\ell,t^{-}}\label{eq:upd-h-2-1}\\
\Delta_{ia}^{\ell,t} & =\partial_{B}^{2}\varphi_{ia}^{\ell,t^{-}}\label{eq:upd-sigma-2-1}\\
m_{ki}^{\ell,t} & =\partial_{H}\psi_{ki}^{\ell,t^{-}}\\
\sigma_{ki}^{\ell,t} & =\partial_{H}^{2}\psi_{ki}^{\ell,t^{-}}\\
V_{ka}^{\ell,t} & =\sum_{i}\left(\left(m_{ki}^{\ell,t}\right)^{2}\Delta_{ia}^{\ell,t}+\sigma_{ki}^{\ell,t}(\hat{x}_{ia}^{\ell,t})^{2}+\sigma_{ki}^{\ell,t}\Delta_{ia}^{\ell,t}\right)\label{eq:upd-V-2-1}\\
\omega_{ka}^{\ell,t} & =\sum_{i}m_{ki}^{\ell,t}\hat{x}_{ia}^{\ell,t}+TODO:onsagsize(x̂)er\label{eq:upd-w-2-1}
\end{align}

\end_inset


\end_layout

\begin_layout Standard
Here 
\begin_inset Formula $V^{\ell}$
\end_inset

 and 
\begin_inset Formula $\omega^{\ell}$
\end_inset

 are computed as a function of the previous layer values 
\begin_inset Formula $V^{\ell-1}$
\end_inset

 and 
\begin_inset Formula $\omega^{\ell-1}$
\end_inset

.
 
\end_layout

\begin_layout Paragraph
Backward pass.
\end_layout

\begin_layout Standard
In the BACKWARD sweep, starting from 
\begin_inset Formula $\ell=L$
\end_inset

 and up to 
\begin_inset Formula $\ell=0$
\end_inset

, we have 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align}
g_{ka}^{\ell,t} & =\partial_{\omega}\varphi_{ka}^{\ell+1,t}\label{eq:upd-g-2-1}\\
\Gamma_{ka}^{\ell,t} & =-\partial_{\omega}^{2}\varphi_{ka}^{\ell+1,t}\label{eq:upd-dwg-2-1}\\
A_{ia}^{\ell,t} & =\sum_{k}\left((m_{ki}^{\ell,t})^{2}+\sigma_{ki}^{\ell,t}\right)\Gamma_{ka}^{\ell,t}-\sigma_{ki}^{\ell,t}\left(g_{ka}^{\ell,t}\right)^{2}\label{eq:upd-A-2-1}\\
B_{ia}^{\ell,t} & =\sum_{k}m_{ki}^{\ell}g_{ka}^{\ell,t}+TODO:onsager\label{eq:upd-B-2-1}\\
G_{ki}^{\ell,t} & =\sum_{a}\left((\hat{x}_{ia}^{\ell,t})^{2}+\Delta_{ia}\right)\Gamma_{ka}^{\ell,t}-\Delta_{ia}\left(g_{ka}^{\ell,t}\right)^{2}\\
H_{ki} & =\sum_{a}\hat{x}_{ia}^{\ell,t}g_{ka}^{\ell,t}+TODO:onsager
\end{align}

\end_inset


\end_layout

\begin_layout Standard
Notice that 
\begin_inset Formula $A^{\ell}$
\end_inset

 and 
\begin_inset Formula $B^{\ell}$
\end_inset

 are computed as as a function of the 
\begin_inset Formula $A^{\ell+1},B^{\ell+1}$
\end_inset

 of the layer above, with the initial condition given by the output 
\begin_inset Formula $\boldsymbol{x}^{L+1}=\boldsymbol{y}$
\end_inset

 on the top layer.
\end_layout

\end_body
\end_document
